# Web Crawler

## Overview
The Web Crawler is a Java program designed to fetch web pages from a specified URL and extract useful information, such as links and keywords, from those pages. It operates by traversing the web pages in a systematic manner, following links, and analyzing content to gather data.

## Features
- Fetch web pages from a given URL
- Extract links from the fetched pages
- Search for specific keywords within the page content
- Multithreaded crawling for improved performance
- Concurrent processing of multiple URLs

## Installation
1. Clone the repository to your local machine.
2. Ensure you have Java installed on your system.
3. Ensure the BASE_URL env is set to the base URL.
4. Build the project.

## Usage
1. Open http://localhost:4567/
2. Use the Web Crawler UI Page to add processes.
![image](https://github.com/Klaus-Mueller/WebCrawlerJava/assets/6924934/8060e860-6e9f-476d-8fc2-6e5de83c7a6b)


## Documentation:
